{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random \n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation ,Dropout\n",
    "from keras.optimizers import gradient_descent_v2\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "intents=json.loads(open('inputs.json').read())\n",
    "words=[]\n",
    "classes =[]\n",
    "documents =[]\n",
    "ignore = ['?',',','.','!',':'] \n",
    "#Data pre processing\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list,intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "words=[lemmatizer.lemmatize(word) for word in words if word not in ignore]\n",
    "words=sorted(set(words))\n",
    "classes=sorted(set(classes))\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))\n",
    "training = []\n",
    "output_empty = [0]*len(classes)\n",
    "for document in documents:\n",
    "    bag=[]\n",
    "    word_patterns = document[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "    output_row=list(output_empty)\n",
    "    output_row[classes.index(document[1])]=1\n",
    "    training.append([bag,output_row])\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "train_x=list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n",
    "\n",
    "#Neural Network model\n",
    "model= Sequential()\n",
    "model.add(Dense(128,input_shape=(len(train_x[0]),),activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]),activation='softmax'))\n",
    "sgd = gradient_descent_v2.SGD(lr=0.01,decay=1e-6,momentum=0.9,nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
    "hist=model.fit(np.array(train_x),np.array(train_y),epochs=250,batch_size=5,verbose=1)\n",
    "model.save('chatbot.model')\n",
    "model.save('chatbot.h5',hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random \n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "intents=json.loads(open('inputs.json').read())\n",
    "words=pickle.load(open('words.pkl','rb'))\n",
    "classes=pickle.load(open('classes.pkl','rb'))\n",
    "model=load_model('chatbot.model')\n",
    "#creating functions for the model to search for patterns and tags in inputs to create relevant responses\n",
    "def clean_up(line):\n",
    "    line_words = nltk.word_tokenize(line)\n",
    "    line_words = [lemmatizer.lemmatize(word) for word in line_words]\n",
    "    return line_words\n",
    "def bag_words(line):\n",
    "    line_words=clean_up(line)\n",
    "    bag=[0]*len(words)\n",
    "    for i in line_words:\n",
    "        for j,word in enumerate(words):\n",
    "            if word==i:\n",
    "                bag[j]=1\n",
    "    return np.array(bag)\n",
    "def predict_class(line):\n",
    "    bag=bag_words(line)\n",
    "    res=model.predict(np.array([bag]))[0]\n",
    "    errort=0.25\n",
    "    results =[[i,r] for i,r in enumerate(res) if r>errort]\n",
    "    results.sort(key=lambda x:x[1],reverse = True)\n",
    "    return_list=[]\n",
    "    for r in results:\n",
    "        return_list.append({'intent':classes[r[0]],'probability':str(r[1])})\n",
    "    return return_list\n",
    "def reponses(intents_list,intents_json):\n",
    "    tag=intents_list[0]['intent']\n",
    "    listintents=intents_json['intents']\n",
    "    for i in listintents:\n",
    "        if i['tag']==tag:\n",
    "            result =random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import playsound\n",
    "from googletrans import Translator, constants\n",
    "from datetime import datetime\n",
    "from gtts import gTTS\n",
    "#Taking user inputs and then generating output for user.\n",
    "print(\"---------------------Ruby is now Online ------------------------\")\n",
    "Name=input(\"May I know your name?\")\n",
    "print(\"Hey {0} this is Ruby from VIT-Bhopal\".format(Name))\n",
    "InputLanguageChoice = input(\"Which language do you want to continue?\\n To continue in English enter en \\n To continue in Telugu enter te \\n To continue in Hindi hi\")\n",
    "    \n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ques = input(\"Query:\")\n",
    "    translator = Translator()\n",
    "    Translation = translator.translate(ques,dest=\"en\",src=InputLanguageChoice)\n",
    "    query=Translation.text\n",
    "    #query = input(\"{name} :\".format(name=Name))\n",
    "    if ques == 'Bye' or ques==\"bye\" or ques =='ok thanks':\n",
    "      \n",
    "      print(\"'Ruby:' Bye\")\n",
    "      \n",
    "      \n",
    "      print(\"For more information please visit: https://admission.vitbhopal.ac.in/\")\n",
    "      break\n",
    "    else:  \n",
    "      ints=predict_class(query)\n",
    "      res=reponses(ints,intents)\n",
    "      translator =Translator()\n",
    "      translation = translator.translate(res, dest=InputLanguageChoice)\n",
    "      print(f\"'Ruby:'{translation.text}\")\n",
    "      tts=gTTS(text=translation.text,lang =InputLanguageChoice)\n",
    "      date_string = datetime.now().strftime(\"%d%m%Y%H%M%S\")\n",
    "      file = \"testel\"+date_string+\".mp3\"\n",
    "      tts.save(file)\n",
    "      playsound.playsound(file)\n",
    "      os.remove(file)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
